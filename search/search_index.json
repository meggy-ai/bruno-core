{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"bruno-core Documentation","text":"<p>Welcome to bruno-core - a modular, extensible foundation for building AI assistants.</p>"},{"location":"#overview","title":"Overview","text":"<p>bruno-core provides clean abstractions, swappable components, and a plugin architecture that enables you to build sophisticated conversational AI systems. It serves as the foundation layer that other Bruno packages build upon.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83e\udde9 Modular Architecture - Clean separation between interfaces, implementations, and plugins</li> <li>\ud83d\udd0c Plugin System - Easy registration via Python entry points</li> <li>\ud83d\udcac Conversation Management - Context windows, sessions, and state persistence</li> <li>\u26a1 Async-First - Built on asyncio for high-performance</li> <li>\ud83c\udfaf Action Execution - Parallel execution, rollback, and chaining</li> <li>\ud83d\udcca Event System - Pub/sub for decoupled communication</li> <li>\ud83e\uddea Well Tested - &gt;80% code coverage</li> <li>\ud83d\udcdd Type Safe - Full type hints and Pydantic validation</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quick Start Guide - Get up and running in minutes</li> <li>Architecture Overview - Understand the system design</li> <li>Installation - Installation instructions</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>Interfaces - Abstract contracts</li> <li>Base Classes - Ready-to-extend implementations</li> <li>Data Models - Pydantic models</li> </ul>"},{"location":"#developer-guides","title":"Developer Guides","text":"<ul> <li>Creating Abilities - Build custom abilities</li> <li>Custom LLM Providers - Integrate any LLM</li> <li>Memory Backends - Custom storage solutions</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Your Application                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u25b2\n                  \u2502 uses\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502             \u2502             \u2502          \u2502\n\u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Custom \u2502  \u2502  Custom  \u2502  \u2502 Custom  \u2502 \u2502  Custom    \u2502\n\u2502  LLM   \u2502  \u2502  Memory  \u2502  \u2502Abilities\u2502 \u2502  Events    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u25b2            \u25b2             \u25b2            \u25b2\n    \u2502            \u2502             \u2502            \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502 implements\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502  bruno-    \u2502\n            \u2502   core     \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#core-components","title":"Core Components","text":""},{"location":"#interfaces","title":"Interfaces","text":"<p>Abstract contracts that define how components interact: - <code>AssistantInterface</code> - Main orchestrator contract - <code>LLMInterface</code> - Language model provider contract - <code>MemoryInterface</code> - Storage backend contract - <code>AbilityInterface</code> - Ability/skill contract - <code>EmbeddingInterface</code> - Vector embedding contract</p>"},{"location":"#base-implementations","title":"Base Implementations","text":"<p>Ready-to-extend base classes: - <code>BaseAssistant</code> - Default assistant implementation - <code>BaseAbility</code> - Ability template with validation - <code>ActionExecutor</code> - Action orchestration - <code>ChainExecutor</code> - Sequential ability execution</p>"},{"location":"#models","title":"Models","text":"<p>Type-safe data structures: - <code>Message</code> - Chat messages with roles - <code>ConversationContext</code> - Message history - <code>AbilityRequest/Response</code> - Ability communication - <code>MemoryEntry</code> - Stored memories</p>"},{"location":"#registry","title":"Registry","text":"<p>Plugin discovery and management: - <code>AbilityRegistry</code> - Discover and load abilities - <code>LLMProviderRegistry</code> - Manage LLM providers - <code>MemoryBackendRegistry</code> - Manage storage backends</p>"},{"location":"#context-management","title":"Context Management","text":"<ul> <li><code>ContextManager</code> - Rolling message windows</li> <li><code>SessionManager</code> - Session lifecycle</li> <li><code>StateManager</code> - Persistent state storage</li> </ul>"},{"location":"#events","title":"Events","text":"<p>Pub/sub event system: - <code>EventBus</code> - Event distribution - <code>EventHandler</code> - Base event handlers - Event types for all system activities</p>"},{"location":"#example-usage","title":"Example Usage","text":"<pre><code>from bruno_core.base import BaseAssistant\nfrom bruno_core.models import Message, MessageRole\n\n# Initialize assistant\nassistant = BaseAssistant(llm=your_llm, memory=your_memory)\nawait assistant.initialize()\n\n# Process message\nmessage = Message(role=MessageRole.USER, content=\"Hello!\")\nresponse = await assistant.process_message(\n    message=message,\n    user_id=\"user-123\",\n    conversation_id=\"conv-456\"\n)\n\nprint(response.text)\n</code></pre>"},{"location":"#package-structure","title":"Package Structure","text":"<pre><code>bruno_core/\n\u251c\u2500\u2500 interfaces/      # Abstract contracts\n\u251c\u2500\u2500 base/           # Base implementations\n\u251c\u2500\u2500 models/         # Data models\n\u251c\u2500\u2500 registry/       # Plugin system\n\u251c\u2500\u2500 context/        # Context management\n\u251c\u2500\u2500 events/         # Event system\n\u251c\u2500\u2500 utils/          # Utilities\n\u2514\u2500\u2500 protocols/      # Type protocols\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>See our Contributing Guide for development setup and guidelines.</p>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"#support","title":"Support","text":"<ul> <li>GitHub Issues</li> <li>GitHub Discussions</li> </ul>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>This document explains the design principles, architecture patterns, and component interactions in bruno-core.</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":""},{"location":"architecture/#1-modularity","title":"1. Modularity","text":"<ul> <li>Separation of Concerns: Each component has a single, well-defined responsibility</li> <li>Loose Coupling: Components interact through interfaces, not concrete implementations</li> <li>High Cohesion: Related functionality is grouped together</li> </ul>"},{"location":"architecture/#2-extensibility","title":"2. Extensibility","text":"<ul> <li>Plugin Architecture: New capabilities via entry points</li> <li>Interface-Based Design: Swap implementations without code changes</li> <li>Base Class Templates: Common functionality in reusable base classes</li> </ul>"},{"location":"architecture/#3-type-safety","title":"3. Type Safety","text":"<ul> <li>Pydantic Models: Runtime validation and serialization</li> <li>Type Hints: Static type checking with mypy</li> <li>Protocol Types: Structural subtyping for flexibility</li> </ul>"},{"location":"architecture/#4-async-first","title":"4. Async-First","text":"<ul> <li>Non-Blocking I/O: All I/O operations are async</li> <li>Concurrent Execution: Parallel ability execution</li> <li>Event-Driven: Async event handlers</li> </ul>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":""},{"location":"architecture/#layer-overview","title":"Layer Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Application Layer                    \u2502\n\u2502         (Your AI Assistant Application)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u25b2\n                      \u2502 uses\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Plugin Layer                         \u2502\n\u2502    (Abilities, LLM Providers, Memory Backends)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u25b2\n                      \u2502 implements\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Base Implementation Layer               \u2502\n\u2502  (BaseAssistant, BaseAbility, ActionExecutor)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u25b2\n                      \u2502 uses\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Interface Layer                      \u2502\n\u2502   (Contracts: LLMInterface, MemoryInterface)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u25b2\n                      \u2502 depends on\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Foundation Layer                     \u2502\n\u2502    (Models, Utils, Registry, Events, Context)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-interfaces-contracts","title":"1. Interfaces (Contracts)","text":"<p>Interfaces define contracts that implementations must fulfill.</p> <p>Key Interfaces: - <code>AssistantInterface</code> - Main orchestrator - <code>LLMInterface</code> - Language model provider - <code>MemoryInterface</code> - Storage backend - <code>AbilityInterface</code> - Ability/skill - <code>EmbeddingInterface</code> - Vector embeddings</p> <p>Design Pattern: Abstract Base Class (ABC)</p> <pre><code>from abc import ABC, abstractmethod\n\nclass LLMInterface(ABC):\n    @abstractmethod\n    async def generate(self, messages, **kwargs) -&gt; str:\n        \"\"\"Generate response from messages.\"\"\"\n        pass\n</code></pre>"},{"location":"architecture/#2-base-implementations","title":"2. Base Implementations","text":"<p>Reusable implementations that handle common patterns.</p> <p>BaseAssistant - The main orchestrator: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         BaseAssistant               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 - llm: LLMInterface                 \u2502\n\u2502 - memory: MemoryInterface           \u2502\n\u2502 - abilities: Dict[str, Ability]     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 + process_message()                 \u2502\n\u2502 + register_ability()                \u2502\n\u2502 + initialize()                      \u2502\n\u2502 + shutdown()                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u251c\u2500\u25ba uses LLMInterface\n         \u251c\u2500\u25ba uses MemoryInterface\n         \u2514\u2500\u25ba manages AbilityInterface\n</code></pre></p> <p>ActionExecutor - Orchestrates ability execution: - Sequential or parallel execution - Rollback on failure - Result aggregation - Statistics tracking</p> <p>ChainExecutor - Sequential ability chains: - Step-by-step execution - Result passing between steps - Conditional branching - Error handling</p>"},{"location":"architecture/#3-models-data-structures","title":"3. Models (Data Structures)","text":"<p>Type-safe data models using Pydantic v2.</p> <p>Message Flow: <pre><code>User Input\n    \u2193\nMessage (role=USER, content=...)\n    \u2193\nConversationContext (messages=[...])\n    \u2193\nLLM Processing\n    \u2193\nAssistantResponse (text=..., actions=[...])\n    \u2193\nOutput to User\n</code></pre></p> <p>Key Models: - <code>Message</code> - Chat messages with roles and metadata - <code>ConversationContext</code> - Collection of messages with context - <code>AssistantResponse</code> - Assistant's response with actions - <code>AbilityRequest/Response</code> - Ability communication - <code>MemoryEntry</code> - Stored memory with importance scoring</p>"},{"location":"architecture/#4-registry-system","title":"4. Registry System","text":"<p>Dynamic plugin discovery and management.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       Plugin Registry              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Entry Points Discovery            \u2502\n\u2502       \u2193                            \u2502\n\u2502  Plugin Validation                 \u2502\n\u2502       \u2193                            \u2502\n\u2502  Instance Creation                 \u2502\n\u2502       \u2193                            \u2502\n\u2502  Lifetime Management               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Entry Point Format: <pre><code>entry_points={\n    \"bruno.abilities\": [\n        \"timer = my_package.abilities:TimerAbility\",\n    ]\n}\n</code></pre></p> <p>Usage: <pre><code>registry = AbilityRegistry()\nregistry.discover_plugins()  # Auto-discover\ntimer = registry.get_instance(\"timer\")\n</code></pre></p>"},{"location":"architecture/#5-context-management","title":"5. Context Management","text":"<p>Manages conversation state and history.</p> <p>ContextManager - Rolling message windows: - Fixed-size message buffer - Automatic memory storage - Compression triggers - Multi-conversation support</p> <p>SessionManager - Session lifecycle: - Session creation/termination - Activity tracking - Timeout handling - Statistics</p> <p>StateManager - Persistent state: - Key-value storage - JSON serialization - Namespace isolation - File or in-memory storage</p>"},{"location":"architecture/#6-event-system","title":"6. Event System","text":"<p>Pub/sub pattern for decoupled communication.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Component A \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502 publish\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     subscribe      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Event Bus   \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 \u2502  Component B \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502 subscribe\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Component C \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Event Types: - Message events (received, sent, processed) - Ability events (executing, executed, failed) - Session events (started, ended) - System events (started, stopped, health)</p>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#message-processing-pipeline","title":"Message Processing Pipeline","text":"<pre><code>1. User Input\n      \u2193\n2. Create Message\n      \u2193\n3. Add to Context\n      \u2193\n4. Retrieve Relevant Memories\n      \u2193\n5. Build LLM Context\n      \u2193\n6. Generate LLM Response\n      \u2193\n7. Detect Abilities\n      \u2193\n8. Execute Abilities (if any)\n      \u2193\n9. Create AssistantResponse\n      \u2193\n10. Store in Memory\n      \u2193\n11. Return Response\n</code></pre>"},{"location":"architecture/#ability-execution-pipeline","title":"Ability Execution Pipeline","text":"<pre><code>1. Detect Ability Keywords\n      \u2193\n2. Create AbilityRequest\n      \u2193\n3. Validate Request\n      \u2193\n4. Execute Action\n      \u2193\n5. Handle Result/Error\n      \u2193\n6. Create ActionResult\n      \u2193\n7. Return to Assistant\n</code></pre>"},{"location":"architecture/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/#1-strategy-pattern","title":"1. Strategy Pattern","text":"<p>Swap algorithms at runtime (LLM providers, memory backends).</p> <pre><code># Different strategies\nassistant = BaseAssistant(llm=OpenAIProvider(...))\nassistant = BaseAssistant(llm=ClaudeProvider(...))\nassistant = BaseAssistant(llm=OllamaProvider(...))\n</code></pre>"},{"location":"architecture/#2-template-method-pattern","title":"2. Template Method Pattern","text":"<p>Base classes define skeleton, subclasses implement specifics.</p> <pre><code>class BaseAbility(AbilityInterface):\n    async def execute(self, request):  # Template\n        if not self.validate_request(request):\n            return error_response()\n        return await self.execute_action(request)  # Subclass implements\n\n    @abstractmethod\n    async def execute_action(self, request):  # To be implemented\n        pass\n</code></pre>"},{"location":"architecture/#3-observer-pattern","title":"3. Observer Pattern","text":"<p>Event system for loose coupling.</p> <pre><code>bus.subscribe(EventType.MESSAGE_RECEIVED, log_handler)\nbus.subscribe(EventType.MESSAGE_RECEIVED, metrics_handler)\nbus.subscribe(EventType.MESSAGE_RECEIVED, analytics_handler)\n</code></pre>"},{"location":"architecture/#4-registry-pattern","title":"4. Registry Pattern","text":"<p>Centralized plugin management.</p> <pre><code>registry = AbilityRegistry()\nregistry.register(\"timer\", TimerAbility)\ntimer = registry.get_instance(\"timer\")\n</code></pre>"},{"location":"architecture/#5-facade-pattern","title":"5. Facade Pattern","text":"<p>BaseAssistant provides simplified interface to complex subsystems.</p> <pre><code># Simple interface\nresponse = await assistant.process_message(message, user_id, conv_id)\n\n# Hides complexity of:\n# - Context management\n# - Memory retrieval\n# - LLM interaction\n# - Ability detection\n# - Ability execution\n# - Event publishing\n</code></pre>"},{"location":"architecture/#extensibility-points","title":"Extensibility Points","text":""},{"location":"architecture/#1-custom-llm-providers","title":"1. Custom LLM Providers","text":"<p>Implement <code>LLMInterface</code>: <pre><code>class CustomLLM(LLMInterface):\n    async def generate(self, messages, **kwargs):\n        # Your implementation\n        pass\n</code></pre></p>"},{"location":"architecture/#2-custom-memory-backends","title":"2. Custom Memory Backends","text":"<p>Implement <code>MemoryInterface</code>: <pre><code>class CustomMemory(MemoryInterface):\n    async def store_message(self, message, user_id, conversation_id):\n        # Your implementation\n        pass\n</code></pre></p>"},{"location":"architecture/#3-custom-abilities","title":"3. Custom Abilities","text":"<p>Extend <code>BaseAbility</code>: <pre><code>class WeatherAbility(BaseAbility):\n    async def execute_action(self, request):\n        # Your implementation\n        pass\n</code></pre></p>"},{"location":"architecture/#4-event-handlers","title":"4. Event Handlers","text":"<p>Subscribe to events: <pre><code>class LoggingHandler(AsyncEventHandler):\n    async def handle(self, event):\n        # Your implementation\n        pass\n</code></pre></p>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/#1-async-operations","title":"1. Async Operations","text":"<p>All I/O is non-blocking: <pre><code># Parallel execution\nresults = await asyncio.gather(\n    llm.generate(messages),\n    memory.retrieve_context(user_id, query),\n    ability.execute(request)\n)\n</code></pre></p>"},{"location":"architecture/#2-context-window-management","title":"2. Context Window Management","text":"<p>Rolling windows prevent unbounded growth: <pre><code>manager = ContextManager(max_messages=20)\n# Automatically keeps only last 20 messages\n</code></pre></p>"},{"location":"architecture/#3-lazy-loading","title":"3. Lazy Loading","text":"<p>Registry creates instances on-demand: <pre><code># Plugin class loaded but not instantiated until needed\nability = registry.get_instance(\"timer\")  # Created here\n</code></pre></p>"},{"location":"architecture/#4-event-bus-efficiency","title":"4. Event Bus Efficiency","text":"<p>Handlers execute concurrently: <pre><code># All handlers run in parallel\nawait bus.publish(event)\n</code></pre></p>"},{"location":"architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"architecture/#1-input-validation","title":"1. Input Validation","text":"<p>All models validate input via Pydantic: <pre><code>message = Message(role=MessageRole.USER, content=user_input)\n# Validated automatically\n</code></pre></p>"},{"location":"architecture/#2-sandboxed-execution","title":"2. Sandboxed Execution","text":"<p>Abilities can't access other abilities' state: <pre><code># Each ability is isolated\nability1 = registry.get_instance(\"timer\")\nability2 = registry.get_instance(\"notes\")\n# No shared state\n</code></pre></p>"},{"location":"architecture/#3-permission-system","title":"3. Permission System","text":"<p>Extend with custom permissions: <pre><code>class SecureAbility(BaseAbility):\n    async def execute_action(self, request):\n        if not self.check_permission(request.user_id):\n            raise PermissionError()\n        # Execute\n</code></pre></p>"},{"location":"architecture/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/#1-unit-tests","title":"1. Unit Tests","text":"<p>Test individual components in isolation: <pre><code># Mock dependencies\nassistant = BaseAssistant(llm=MockLLM(), memory=MockMemory())\n</code></pre></p>"},{"location":"architecture/#2-integration-tests","title":"2. Integration Tests","text":"<p>Test component interactions: <pre><code># Real implementations\nassistant = BaseAssistant(llm=real_llm, memory=real_memory)\nresponse = await assistant.process_message(...)\n</code></pre></p>"},{"location":"architecture/#3-mock-implementations","title":"3. Mock Implementations","text":"<p>Provided in <code>tests/conftest.py</code>: - <code>MockLLM</code> - Predictable LLM responses - <code>MockMemory</code> - In-memory storage - <code>MockAbility</code> - Test ability behavior</p>"},{"location":"architecture/#best-practices","title":"Best Practices","text":""},{"location":"architecture/#1-use-type-hints","title":"1. Use Type Hints","text":"<pre><code>async def process(message: Message) -&gt; AssistantResponse:\n    ...\n</code></pre>"},{"location":"architecture/#2-handle-errors-gracefully","title":"2. Handle Errors Gracefully","text":"<pre><code>try:\n    response = await llm.generate(messages)\nexcept Exception as e:\n    logger.error(\"llm_failed\", error=str(e))\n    return error_response()\n</code></pre>"},{"location":"architecture/#3-log-structured-data","title":"3. Log Structured Data","text":"<pre><code>logger.info(\"message_processed\", user_id=user_id, duration=elapsed)\n</code></pre>"},{"location":"architecture/#4-use-context-managers","title":"4. Use Context Managers","text":"<pre><code>async with assistant:\n    response = await assistant.process_message(message)\n</code></pre>"},{"location":"architecture/#5-validate-early","title":"5. Validate Early","text":"<pre><code>def validate_request(self, request: AbilityRequest) -&gt; bool:\n    # Fail fast\n    if not request.parameters:\n        return False\n    return True\n</code></pre>"},{"location":"architecture/#future-enhancements","title":"Future Enhancements","text":"<p>Planned features: - [ ] Streaming response support - [ ] Distributed execution - [ ] Plugin versioning &amp; dependencies - [ ] Hot reloading of abilities - [ ] Enhanced observability - [ ] Rate limiting &amp; quotas - [ ] Multi-modal support</p> <p>For implementation details, see the API Reference.</p>"},{"location":"project_overview/","title":"\ud83c\udf89 Bruno Core - Project Complete!","text":""},{"location":"project_overview/#status-all-9-phases-completed","title":"Status: ALL 9 PHASES COMPLETED \u2705","text":"<p>Completion Date: December 8, 2025 Total Implementation Time: ~20 hours across all phases Package Version: 0.1.0-alpha (ready for release)</p>"},{"location":"project_overview/#project-overview","title":"\ud83d\udcca Project Overview","text":"<p>Bruno Core is a modular, extensible foundation package for building AI assistants. It provides clean interfaces, base implementations, and utilities that enable the Bruno ecosystem.</p>"},{"location":"project_overview/#design-goals","title":"Design Goals \u2705","text":"<ul> <li>\u2705 Modular architecture with clear interfaces</li> <li>\u2705 Plugin system for extensibility</li> <li>\u2705 Async-first for performance</li> <li>\u2705 Type-safe with comprehensive type hints</li> <li>\u2705 Well-documented with examples</li> <li>\u2705 Production-ready with CI/CD</li> </ul>"},{"location":"project_overview/#all-phases-complete","title":"\u2705 All Phases Complete","text":"Phase Status Description 1 \u2705 Foundation &amp; Core Infrastructure 2 \u2705 Core Interfaces (Contracts) 3 \u2705 Base Implementations 4 \u2705 Plugin Registry System 5 \u2705 Context Management 6 \u2705 Event System 7 \u2705 Testing Infrastructure 8 \u2705 Documentation 9 \u2705 CI/CD &amp; Release"},{"location":"project_overview/#whats-included","title":"\ud83d\udce6 What's Included","text":""},{"location":"project_overview/#core-package-bruno_core","title":"Core Package (bruno_core/)","text":"<ul> <li>Interfaces: 6 abstract interfaces (Assistant, LLM, Memory, Ability, Embedding, Stream)</li> <li>Base Classes: 4 base implementations (BaseAssistant, BaseAbility, ActionExecutor, ChainExecutor)</li> <li>Models: 6 Pydantic model modules (Message, Context, Response, Memory, Ability, Config)</li> <li>Registry: 3 registry systems (Ability, LLM, Memory)</li> <li>Context: 3 context managers (Context, Session, State)</li> <li>Events: Complete event system (EventBus, handlers, event types)</li> <li>Utils: 6 utility modules (exceptions, logging, validation, async, text, config)</li> <li>Protocols: Runtime-checkable protocols for duck typing</li> </ul>"},{"location":"project_overview/#tests-tests","title":"Tests (tests/)","text":"<ul> <li>Unit Tests: 6 test modules covering all components</li> <li>Integration Tests: End-to-end workflow testing</li> <li>Fixtures: Comprehensive mock implementations</li> <li>Coverage: 80%+ target with automated reporting</li> </ul>"},{"location":"project_overview/#documentation-docs","title":"Documentation (docs/)","text":"<ul> <li>Main Docs: index, quickstart, architecture</li> <li>Guides: Creating abilities, custom LLM, memory backends</li> <li>API Reference: Structure ready for auto-generation</li> <li>~3,300 lines of comprehensive documentation</li> </ul>"},{"location":"project_overview/#examples-examples","title":"Examples (examples/)","text":"<ul> <li>5 Working Examples: Basic assistant, custom ability, custom LLM, memory usage, event handling</li> <li>~1,300 lines of practical code</li> <li>No external dependencies: Uses mocks for portability</li> </ul>"},{"location":"project_overview/#cicd-githubworkflows","title":"CI/CD (.github/workflows/)","text":"<ul> <li>Test Workflow: Multi-OS, multi-Python testing</li> <li>Lint Workflow: Code quality and security checks</li> <li>Publish Workflow: Automated PyPI publishing</li> <li>Docs Workflow: GitHub Pages deployment</li> </ul>"},{"location":"project_overview/#development-tools-scripts","title":"Development Tools (scripts/)","text":"<ul> <li>release.py: Automated version bumping and release</li> <li>check_release.py: Pre-release validation</li> <li>setup_dev.py: Development environment setup</li> </ul>"},{"location":"project_overview/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"project_overview/#installation-after-first-release","title":"Installation (after first release)","text":"<pre><code>pip install bruno-core\n</code></pre>"},{"location":"project_overview/#basic-usage","title":"Basic Usage","text":"<pre><code>from bruno_core.base import BaseAssistant\nfrom bruno_core.models import Message, MessageRole\n\n# Create assistant with your LLM and memory implementations\nassistant = BaseAssistant(llm=your_llm, memory=your_memory)\nawait assistant.initialize()\n\n# Process messages\nmessage = Message(role=MessageRole.USER, content=\"Hello!\")\nresponse = await assistant.process_message(\n    message=message,\n    user_id=\"user123\",\n    conversation_id=\"conv456\"\n)\n\nprint(response.text)\n</code></pre>"},{"location":"project_overview/#extending-bruno-core","title":"Extending Bruno Core","text":"<p>Custom Ability: <pre><code>from bruno_core.base import BaseAbility\n\nclass MyAbility(BaseAbility):\n    def get_metadata(self):\n        return AbilityMetadata(name=\"my-ability\", ...)\n\n    async def execute_action(self, request):\n        # Your implementation\n        return AbilityResponse(...)\n</code></pre></p> <p>Custom LLM: <pre><code>from bruno_core.interfaces import LLMInterface\n\nclass MyLLM(LLMInterface):\n    async def generate(self, messages, **kwargs):\n        # Your LLM integration\n        return \"response\"\n</code></pre></p> <p>See <code>examples/</code> for complete working examples!</p>"},{"location":"project_overview/#project-metrics","title":"\ud83d\udcc8 Project Metrics","text":""},{"location":"project_overview/#code-statistics","title":"Code Statistics","text":"Metric Count Python modules 35+ Test files 7 Documentation files 12+ Example files 6 Total lines of code ~15,000+"},{"location":"project_overview/#quality-metrics","title":"Quality Metrics","text":"Metric Status Test coverage 80%+ \u2705 Type hints 100% \u2705 Documentation Complete \u2705 Python versions 3.8-3.12 \u2705 OS support Linux, Windows, macOS \u2705"},{"location":"project_overview/#ready-for","title":"\ud83c\udfaf Ready For","text":""},{"location":"project_overview/#production-use","title":"\u2705 Production Use","text":"<ul> <li>Stable API</li> <li>Comprehensive testing</li> <li>Error handling</li> <li>Performance optimized</li> <li>Security scanned</li> </ul>"},{"location":"project_overview/#ecosystem-development","title":"\u2705 Ecosystem Development","text":"<ul> <li>bruno-llm: LLM provider implementations (OpenAI, Claude, Ollama, etc.)</li> <li>bruno-memory: Memory backend implementations (PostgreSQL, Redis, ChromaDB, etc.)</li> <li>bruno-abilities: Pre-built abilities (music, calendar, notes, timers, etc.)</li> <li>bruno-pa: Personal assistant application</li> </ul>"},{"location":"project_overview/#community-contributions","title":"\u2705 Community Contributions","text":"<ul> <li>Clear contribution guidelines</li> <li>Automated quality checks</li> <li>Documentation for developers</li> <li>Example implementations</li> <li>Pre-commit hooks</li> </ul>"},{"location":"project_overview/#release-process","title":"\ud83d\udd04 Release Process","text":""},{"location":"project_overview/#creating-the-first-release-v010","title":"Creating the First Release (v0.1.0)","text":"<ol> <li> <p>Validate everything works: <pre><code>python scripts/check_release.py\n</code></pre></p> </li> <li> <p>Create release: <pre><code>python scripts/release.py minor  # Creates v0.1.0\n</code></pre></p> </li> <li> <p>Update CHANGELOG.md with release details</p> </li> <li> <p>Push to GitHub: <pre><code>git push origin main\ngit push origin v0.1.0\n</code></pre></p> </li> <li> <p>Create GitHub Release (triggers PyPI publish)</p> </li> <li>Go to GitHub \u2192 Releases \u2192 Create new release</li> <li>Tag: v0.1.0</li> <li>Title: Bruno Core v0.1.0</li> <li>Description: Initial release</li> <li>Publish release</li> </ol>"},{"location":"project_overview/#github-secrets-required","title":"GitHub Secrets Required","text":"<ul> <li><code>PYPI_API_TOKEN</code> - PyPI API token for publishing</li> <li><code>TEST_PYPI_API_TOKEN</code> - Test PyPI token for testing</li> <li><code>CODECOV_TOKEN</code> - Codecov token for coverage</li> </ul>"},{"location":"project_overview/#documentation-links","title":"\ud83d\udcda Documentation Links","text":"<ul> <li>README.md - Project overview and quick start</li> <li>docs/index.md - Main documentation entry</li> <li>docs/quickstart.md - Getting started guide</li> <li>docs/architecture.md - System design</li> <li>docs/guides/ - Implementation guides</li> <li>examples/ - Working code examples</li> <li>CONTRIBUTING.md - Contribution guidelines</li> <li>CHANGELOG.md - Version history</li> </ul>"},{"location":"project_overview/#development","title":"\ud83d\udee0\ufe0f Development","text":""},{"location":"project_overview/#setup-development-environment","title":"Setup Development Environment","text":"<pre><code>git clone https://github.com/meggy-ai/bruno-core.git\ncd bruno-core\npython scripts/setup_dev.py\n</code></pre>"},{"location":"project_overview/#run-tests","title":"Run Tests","text":"<pre><code>pytest tests/ -v --cov=bruno_core\n</code></pre>"},{"location":"project_overview/#check-code-quality","title":"Check Code Quality","text":"<pre><code>python scripts/check_release.py\n</code></pre>"},{"location":"project_overview/#build-documentation","title":"Build Documentation","text":"<pre><code>mkdocs serve  # View at http://127.0.0.1:8000\n</code></pre>"},{"location":"project_overview/#key-learnings-best-practices","title":"\ud83c\udf93 Key Learnings &amp; Best Practices","text":""},{"location":"project_overview/#architecture-decisions","title":"Architecture Decisions","text":"<ul> <li>Async-first: All I/O operations are async for performance</li> <li>Interface-driven: Clear contracts enable extensibility</li> <li>Registry pattern: Dynamic plugin discovery and registration</li> <li>Event-driven: Decoupled components communicate via events</li> <li>Type-safe: Pydantic models and type hints throughout</li> </ul>"},{"location":"project_overview/#testing-strategy","title":"Testing Strategy","text":"<ul> <li>Unit tests: Test components in isolation</li> <li>Integration tests: Test component interactions</li> <li>Mock fixtures: Reusable test doubles</li> <li>Coverage target: 80%+ for production readiness</li> </ul>"},{"location":"project_overview/#documentation-approach","title":"Documentation Approach","text":"<ul> <li>Multiple levels: Quick start \u2192 Guides \u2192 API reference</li> <li>Code examples: Every feature has a working example</li> <li>Real-world patterns: Show actual implementation patterns</li> <li>Troubleshooting: Common issues and solutions included</li> </ul>"},{"location":"project_overview/#cicd-pipeline","title":"CI/CD Pipeline","text":"<ul> <li>Multi-matrix testing: Test all Python versions and OSes</li> <li>Quality gates: Automated code quality checks</li> <li>Security scanning: Dependency and code security checks</li> <li>Automated deployment: Push to deploy docs and packages</li> </ul>"},{"location":"project_overview/#success-criteria-all-met","title":"\ud83c\udf1f Success Criteria - All Met!","text":"Criteria Status Notes Modular design \u2705 Clean separation of concerns Extensible \u2705 Plugin system with registries Type-safe \u2705 100% type hints, Pydantic models Well-tested \u2705 80%+ coverage, CI/CD Documented \u2705 Comprehensive guides + examples Production-ready \u2705 Error handling, logging, monitoring CI/CD \u2705 Automated testing, publishing, docs Ecosystem foundation \u2705 Ready for extension packages"},{"location":"project_overview/#next-steps","title":"\ud83d\ude80 Next Steps","text":""},{"location":"project_overview/#immediate-before-v010-release","title":"Immediate (Before v0.1.0 Release)","text":"<ol> <li>\u2705 All implementation complete</li> <li>\u23f3 Configure GitHub secrets</li> <li>\u23f3 Create first release (v0.1.0)</li> <li>\u23f3 Publish to PyPI</li> <li>\u23f3 Deploy documentation to GitHub Pages</li> </ol>"},{"location":"project_overview/#short-term-v01x","title":"Short-term (v0.1.x)","text":"<ul> <li>Gather community feedback</li> <li>Fix any critical bugs</li> <li>Add more examples</li> <li>Improve documentation based on feedback</li> </ul>"},{"location":"project_overview/#medium-term-v020","title":"Medium-term (v0.2.0)","text":"<ul> <li>Start bruno-llm package (LLM providers)</li> <li>Start bruno-memory package (Memory backends)</li> <li>Start bruno-abilities package (Pre-built abilities)</li> <li>API refinements based on real-world usage</li> </ul>"},{"location":"project_overview/#long-term-v100","title":"Long-term (v1.0.0)","text":"<ul> <li>API stability guarantee</li> <li>Production deployments</li> <li>bruno-pa application</li> <li>Full ecosystem complete</li> </ul>"},{"location":"project_overview/#support-community","title":"\ud83d\udcde Support &amp; Community","text":"<ul> <li>GitHub: https://github.com/meggy-ai/bruno-core</li> <li>Issues: Bug reports and feature requests</li> <li>Discussions: Questions and community chat</li> <li>Documentation: https://meggy-ai.github.io/bruno-core (after deploy)</li> </ul>"},{"location":"project_overview/#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p>Built with reference to the original monolithic <code>old_code</code> implementation, transformed into a clean, modular, production-ready foundation package.</p> <p>Bruno Core v0.1.0 - Ready for the World! \ud83c\udf0d</p> <p>The foundation is laid. The ecosystem awaits. Let's build something amazing! \ud83d\ude80</p>"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>Get started with bruno-core in minutes.</p>"},{"location":"quickstart/#installation","title":"Installation","text":""},{"location":"quickstart/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<pre><code>pip install bruno-core\n</code></pre>"},{"location":"quickstart/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/meggy-ai/bruno-core.git\ncd bruno-core\npip install -e .\n</code></pre>"},{"location":"quickstart/#development-installation","title":"Development Installation","text":"<pre><code>pip install -e \".[dev]\"  # Includes test dependencies\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"quickstart/#1-create-mock-implementations","title":"1. Create Mock Implementations","text":"<p>For this quickstart, we'll create simple mock implementations. In production, use real implementations from bruno-llm and bruno-memory packages.</p> <pre><code># mock_llm.py\nfrom bruno_core.interfaces import LLMInterface\nfrom bruno_core.models import Message\n\nclass MockLLM(LLMInterface):\n    async def generate(self, messages: list[Message], **kwargs) -&gt; str:\n        return \"Hello! I'm a mock assistant.\"\n\n    async def stream(self, messages: list[Message], **kwargs):\n        response = await self.generate(messages, **kwargs)\n        for char in response:\n            yield char\n\n    def get_token_count(self, text: str) -&gt; int:\n        return len(text.split())\n\n    def list_models(self) -&gt; list[str]:\n        return [\"mock-model\"]\n</code></pre> <pre><code># mock_memory.py\nfrom bruno_core.interfaces import MemoryInterface\nfrom bruno_core.models import Message, MemoryEntry\n\nclass MockMemory(MemoryInterface):\n    def __init__(self):\n        self.messages = {}\n\n    async def store_message(self, message: Message, user_id: str, conversation_id: str):\n        key = f\"{user_id}:{conversation_id}\"\n        if key not in self.messages:\n            self.messages[key] = []\n        self.messages[key].append(message)\n\n    async def retrieve_context(self, user_id: str, query: str, limit: int = 10, conversation_id: str = None):\n        key = f\"{user_id}:{conversation_id}\"\n        return self.messages.get(key, [])[-limit:]\n\n    async def search_memories(self, user_id: str, query: str, limit: int = 5):\n        return []\n\n    async def clear_conversation(self, user_id: str, conversation_id: str):\n        key = f\"{user_id}:{conversation_id}\"\n        self.messages.pop(key, None)\n</code></pre>"},{"location":"quickstart/#2-create-an-assistant","title":"2. Create an Assistant","text":"<pre><code># main.py\nimport asyncio\nfrom bruno_core.base import BaseAssistant\nfrom bruno_core.models import Message, MessageRole\nfrom mock_llm import MockLLM\nfrom mock_memory import MockMemory\n\nasync def main():\n    # Initialize components\n    llm = MockLLM()\n    memory = MockMemory()\n\n    # Create assistant\n    assistant = BaseAssistant(llm=llm, memory=memory)\n    await assistant.initialize()\n\n    # Process a message\n    user_message = Message(\n        role=MessageRole.USER,\n        content=\"Hello, how are you?\"\n    )\n\n    response = await assistant.process_message(\n        message=user_message,\n        user_id=\"user-123\",\n        conversation_id=\"conv-456\"\n    )\n\n    print(f\"Assistant: {response.text}\")\n\n    # Cleanup\n    await assistant.shutdown()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"quickstart/#3-run-it","title":"3. Run It","text":"<pre><code>python main.py\n</code></pre> <p>Output: <pre><code>Assistant: Hello! I'm a mock assistant.\n</code></pre></p>"},{"location":"quickstart/#adding-abilities","title":"Adding Abilities","text":"<p>Abilities add functionality to your assistant. Here's how to create and register one:</p> <pre><code>from bruno_core.base import BaseAbility\nfrom bruno_core.models import AbilityMetadata, AbilityRequest, AbilityResponse\n\nclass GreetingAbility(BaseAbility):\n    def get_metadata(self) -&gt; AbilityMetadata:\n        return AbilityMetadata(\n            name=\"greeting\",\n            description=\"Greet users warmly\",\n            version=\"1.0.0\",\n            examples=[\"say hello\", \"greet me\"]\n        )\n\n    async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n        return AbilityResponse(\n            request_id=request.id,\n            ability_name=\"greeting\",\n            action=request.action,\n            success=True,\n            message=f\"Hello {request.user_id}! Nice to meet you!\",\n            data={\"greeted\": True}\n        )\n\n    def get_supported_actions(self) -&gt; list[str]:\n        return [\"greet\", \"hello\"]\n\n# Register ability\ngreeting = GreetingAbility()\nawait assistant.register_ability(greeting)\n</code></pre>"},{"location":"quickstart/#using-the-registry-system","title":"Using the Registry System","text":"<p>For plugin-based architecture, register abilities via entry points:</p> <pre><code># setup.py\nfrom setuptools import setup\n\nsetup(\n    name=\"my-bruno-plugins\",\n    packages=[\"my_abilities\"],\n    entry_points={\n        \"bruno.abilities\": [\n            \"greeting = my_abilities.greeting:GreetingAbility\",\n            \"timer = my_abilities.timer:TimerAbility\",\n        ]\n    }\n)\n</code></pre> <p>Then discover plugins automatically:</p> <pre><code>from bruno_core.registry import AbilityRegistry\n\nregistry = AbilityRegistry()\nregistry.discover_plugins()  # Auto-discovers from entry points\n\n# Get instances\ngreeting = registry.get_instance(\"greeting\")\nawait assistant.register_ability(greeting)\n</code></pre>"},{"location":"quickstart/#configuration","title":"Configuration","text":"<p>Use configuration models for type-safe settings:</p> <pre><code>from bruno_core.models import BrunoConfig, LLMConfig, MemoryConfig\n\nconfig = BrunoConfig(\n    llm=LLMConfig(\n        provider=\"openai\",\n        model=\"gpt-4\",\n        temperature=0.7,\n        max_tokens=1000\n    ),\n    memory=MemoryConfig(\n        backend=\"sqlite\",\n        max_context_messages=20,\n        database_path=\"./bruno.db\"\n    )\n)\n</code></pre>"},{"location":"quickstart/#event-system","title":"Event System","text":"<p>Subscribe to events for monitoring and logging:</p> <pre><code>from bruno_core.events import EventBus, EventType\n\nbus = EventBus(enable_history=True)\n\nasync def on_message(event):\n    print(f\"Message received: {event.message_id}\")\n\nbus.subscribe(EventType.MESSAGE_RECEIVED, on_message)\n\n# Events are published automatically by BaseAssistant\n# Or publish manually:\nawait bus.publish(message_event)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Architecture Guide to understand the design</li> <li>Learn how to Create Custom Abilities</li> <li>Integrate your own LLM Provider</li> <li>Build a Custom Memory Backend</li> <li>Check out Examples for more patterns</li> </ul>"},{"location":"quickstart/#common-issues","title":"Common Issues","text":""},{"location":"quickstart/#import-errors","title":"Import Errors","text":"<p>If you see import errors, ensure bruno-core is installed: <pre><code>pip list | grep bruno-core\n</code></pre></p>"},{"location":"quickstart/#async-errors","title":"Async Errors","text":"<p>All bruno-core APIs are async. Always use <code>await</code> and run within an event loop: <pre><code>import asyncio\nasyncio.run(main())\n</code></pre></p>"},{"location":"quickstart/#type-errors","title":"Type Errors","text":"<p>bruno-core uses type hints. Install mypy for type checking: <pre><code>pip install mypy\nmypy your_code.py\n</code></pre></p>"},{"location":"quickstart/#production-usage","title":"Production Usage","text":"<p>For production deployments:</p> <ol> <li>Use real LLM providers (bruno-llm package)</li> <li>Use persistent memory (bruno-memory package)</li> <li>Enable structured logging</li> <li>Add error handling and retries</li> <li>Monitor with the event system</li> <li>Use configuration files</li> </ol> <p>See our deployment examples for production patterns.</p>"},{"location":"guides/creating_abilities/","title":"Creating Custom Abilities","text":"<p>Learn how to create custom abilities that extend your assistant's capabilities.</p>"},{"location":"guides/creating_abilities/#overview","title":"Overview","text":"<p>Abilities are pluggable skills that your assistant can execute. They follow a simple pattern: 1. Define metadata (name, description, parameters) 2. Implement execution logic 3. Return structured responses</p>"},{"location":"guides/creating_abilities/#basic-ability","title":"Basic Ability","text":""},{"location":"guides/creating_abilities/#step-1-extend-baseability","title":"Step 1: Extend BaseAbility","text":"<pre><code>from bruno_core.base import BaseAbility\nfrom bruno_core.models import (\n    AbilityMetadata,\n    AbilityParameter,\n    AbilityRequest,\n    AbilityResponse,\n)\n\nclass WeatherAbility(BaseAbility):\n    \"\"\"Get weather information for locations.\"\"\"\n\n    def get_metadata(self) -&gt; AbilityMetadata:\n        return AbilityMetadata(\n            name=\"weather\",\n            description=\"Get current weather and forecasts\",\n            version=\"1.0.0\",\n            parameters=[\n                AbilityParameter(\n                    name=\"location\",\n                    type=\"string\",\n                    description=\"City or location name\",\n                    required=True,\n                ),\n                AbilityParameter(\n                    name=\"units\",\n                    type=\"string\",\n                    description=\"Temperature units\",\n                    required=False,\n                    allowed_values=[\"celsius\", \"fahrenheit\"],\n                    default_value=\"celsius\",\n                ),\n            ],\n            examples=[\n                \"What's the weather in London?\",\n                \"Get weather for New York\",\n                \"Weather forecast for Tokyo\",\n            ],\n        )\n\n    async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n        \"\"\"Execute the weather lookup.\"\"\"\n        location = request.parameters.get(\"location\")\n        units = request.parameters.get(\"units\", \"celsius\")\n\n        try:\n            # Your implementation here\n            weather_data = await self._fetch_weather(location, units)\n\n            return AbilityResponse(\n                request_id=request.id,\n                ability_name=\"weather\",\n                action=request.action,\n                success=True,\n                message=f\"Weather in {location}: {weather_data['description']}\",\n                data=weather_data,\n            )\n        except Exception as e:\n            return AbilityResponse(\n                request_id=request.id,\n                ability_name=\"weather\",\n                action=request.action,\n                success=False,\n                error=str(e),\n            )\n\n    def get_supported_actions(self) -&gt; list[str]:\n        \"\"\"Actions this ability can handle.\"\"\"\n        return [\"get_weather\", \"weather_forecast\", \"current_weather\"]\n\n    async def _fetch_weather(self, location: str, units: str) -&gt; dict:\n        \"\"\"Fetch weather from API.\"\"\"\n        # Implementation details\n        pass\n</code></pre>"},{"location":"guides/creating_abilities/#step-2-register-the-ability","title":"Step 2: Register the Ability","text":"<pre><code>from bruno_core.base import BaseAssistant\n\n# Create and register\nweather = WeatherAbility()\nawait assistant.register_ability(weather)\n</code></pre>"},{"location":"guides/creating_abilities/#ability-lifecycle","title":"Ability Lifecycle","text":""},{"location":"guides/creating_abilities/#initialization","title":"Initialization","text":"<pre><code>class MyAbility(BaseAbility):\n    async def initialize(self):\n        \"\"\"Called when ability is registered.\"\"\"\n        await super().initialize()\n        # Your setup code\n        self.api_client = ApiClient()\n        self.cache = {}\n</code></pre>"},{"location":"guides/creating_abilities/#shutdown","title":"Shutdown","text":"<pre><code>    async def shutdown(self):\n        \"\"\"Called when ability is unregistered.\"\"\"\n        # Cleanup code\n        await self.api_client.close()\n        await super().shutdown()\n</code></pre>"},{"location":"guides/creating_abilities/#health-check","title":"Health Check","text":"<pre><code>    async def health_check(self) -&gt; dict:\n        \"\"\"Check if ability is healthy.\"\"\"\n        health = await super().health_check()\n        health[\"api_status\"] = await self.api_client.ping()\n        return health\n</code></pre>"},{"location":"guides/creating_abilities/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/creating_abilities/#custom-validation","title":"Custom Validation","text":"<pre><code>class ValidatedAbility(BaseAbility):\n    def validate_request(self, request: AbilityRequest) -&gt; bool:\n        \"\"\"Custom validation logic.\"\"\"\n        if not super().validate_request(request):\n            return False\n\n        # Custom checks\n        location = request.parameters.get(\"location\")\n        if not location or len(location) &lt; 2:\n            return False\n\n        return True\n</code></pre>"},{"location":"guides/creating_abilities/#rollback-support","title":"Rollback Support","text":"<pre><code>class TransactionalAbility(BaseAbility):\n    async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n        # Store rollback info\n        self.last_action = {\n            \"type\": request.action,\n            \"data\": {\"old_value\": current_value},\n        }\n\n        # Execute\n        result = await self._do_action(request)\n        return result\n\n    async def rollback(self, request: AbilityRequest) -&gt; None:\n        \"\"\"Undo the last action.\"\"\"\n        if self.last_action:\n            await self._restore(self.last_action[\"data\"])\n</code></pre>"},{"location":"guides/creating_abilities/#state-management","title":"State Management","text":"<pre><code>class StatefulAbility(BaseAbility):\n    def __init__(self):\n        super().__init__()\n        self.state = {}\n\n    async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n        user_id = request.user_id\n\n        # Get user-specific state\n        user_state = self.state.get(user_id, {})\n\n        # Update state\n        user_state[\"last_action\"] = request.action\n        self.state[user_id] = user_state\n\n        # Execute\n        return await self._process(request, user_state)\n</code></pre>"},{"location":"guides/creating_abilities/#ability-patterns","title":"Ability Patterns","text":""},{"location":"guides/creating_abilities/#api-integration","title":"API Integration","text":"<pre><code>import aiohttp\n\nclass ApiAbility(BaseAbility):\n    def __init__(self, api_key: str):\n        super().__init__()\n        self.api_key = api_key\n        self.session = None\n\n    async def initialize(self):\n        await super().initialize()\n        self.session = aiohttp.ClientSession(\n            headers={\"Authorization\": f\"Bearer {self.api_key}\"}\n        )\n\n    async def shutdown(self):\n        if self.session:\n            await self.session.close()\n        await super().shutdown()\n\n    async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n        async with self.session.get(self.api_url) as resp:\n            data = await resp.json()\n\n        return AbilityResponse(\n            request_id=request.id,\n            ability_name=self._metadata.name,\n            action=request.action,\n            success=True,\n            data=data,\n        )\n</code></pre>"},{"location":"guides/creating_abilities/#database-operations","title":"Database Operations","text":"<pre><code>import asyncpg\n\nclass DatabaseAbility(BaseAbility):\n    def __init__(self, db_url: str):\n        super().__init__()\n        self.db_url = db_url\n        self.pool = None\n\n    async def initialize(self):\n        await super().initialize()\n        self.pool = await asyncpg.create_pool(self.db_url)\n\n    async def shutdown(self):\n        if self.pool:\n            await self.pool.close()\n        await super().shutdown()\n\n    async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n        query = request.parameters.get(\"query\")\n\n        async with self.pool.acquire() as conn:\n            result = await conn.fetch(query)\n\n        return AbilityResponse(\n            request_id=request.id,\n            ability_name=self._metadata.name,\n            action=request.action,\n            success=True,\n            data={\"rows\": [dict(r) for r in result]},\n        )\n</code></pre>"},{"location":"guides/creating_abilities/#file-operations","title":"File Operations","text":"<pre><code>import aiofiles\n\nclass FileAbility(BaseAbility):\n    async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n        action = request.action\n        file_path = request.parameters.get(\"path\")\n\n        if action == \"read\":\n            async with aiofiles.open(file_path, 'r') as f:\n                content = await f.read()\n            return self._success_response(request, {\"content\": content})\n\n        elif action == \"write\":\n            content = request.parameters.get(\"content\")\n            async with aiofiles.open(file_path, 'w') as f:\n                await f.write(content)\n            return self._success_response(request, {\"written\": True})\n</code></pre>"},{"location":"guides/creating_abilities/#scheduled-tasks","title":"Scheduled Tasks","text":"<pre><code>import asyncio\n\nclass ScheduledAbility(BaseAbility):\n    def __init__(self):\n        super().__init__()\n        self.tasks = {}\n\n    async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n        action = request.action\n\n        if action == \"schedule\":\n            delay = request.parameters.get(\"delay\", 60)\n            task_id = str(uuid.uuid4())\n\n            task = asyncio.create_task(self._run_after_delay(delay, request))\n            self.tasks[task_id] = task\n\n            return self._success_response(request, {\"task_id\": task_id})\n\n        elif action == \"cancel\":\n            task_id = request.parameters.get(\"task_id\")\n            task = self.tasks.get(task_id)\n            if task:\n                task.cancel()\n                return self._success_response(request, {\"cancelled\": True})\n\n    async def _run_after_delay(self, delay: int, original_request: AbilityRequest):\n        await asyncio.sleep(delay)\n        # Execute the scheduled action\n        await self._do_scheduled_action(original_request)\n</code></pre>"},{"location":"guides/creating_abilities/#plugin-registration","title":"Plugin Registration","text":""},{"location":"guides/creating_abilities/#via-entry-points","title":"Via Entry Points","text":"<pre><code># setup.py\nfrom setuptools import setup\n\nsetup(\n    name=\"my-bruno-abilities\",\n    packages=[\"my_abilities\"],\n    entry_points={\n        \"bruno.abilities\": [\n            \"weather = my_abilities.weather:WeatherAbility\",\n            \"timer = my_abilities.timer:TimerAbility\",\n            \"notes = my_abilities.notes:NotesAbility\",\n        ]\n    }\n)\n</code></pre>"},{"location":"guides/creating_abilities/#manual-registration","title":"Manual Registration","text":"<pre><code>from bruno_core.registry import AbilityRegistry\n\nregistry = AbilityRegistry()\nregistry.register(\n    name=\"weather\",\n    plugin_class=WeatherAbility,\n    version=\"1.0.0\",\n    metadata={\"category\": \"utilities\"}\n)\n</code></pre>"},{"location":"guides/creating_abilities/#testing-abilities","title":"Testing Abilities","text":"<pre><code>import pytest\nfrom bruno_core.models import AbilityRequest\n\n@pytest.mark.asyncio\nasync def test_weather_ability():\n    ability = WeatherAbility()\n    await ability.initialize()\n\n    request = AbilityRequest(\n        ability_name=\"weather\",\n        action=\"get_weather\",\n        parameters={\"location\": \"London\", \"units\": \"celsius\"},\n        user_id=\"test-user\",\n    )\n\n    response = await ability.execute(request)\n\n    assert response.success is True\n    assert \"weather\" in response.data\n\n    await ability.shutdown()\n</code></pre>"},{"location":"guides/creating_abilities/#best-practices","title":"Best Practices","text":""},{"location":"guides/creating_abilities/#1-use-type-hints","title":"1. Use Type Hints","text":"<pre><code>async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n    location: str = request.parameters.get(\"location\")\n    units: str = request.parameters.get(\"units\", \"celsius\")\n</code></pre>"},{"location":"guides/creating_abilities/#2-validate-input","title":"2. Validate Input","text":"<pre><code>def validate_request(self, request: AbilityRequest) -&gt; bool:\n    if not super().validate_request(request):\n        return False\n\n    location = request.parameters.get(\"location\")\n    if not location or not isinstance(location, str):\n        return False\n\n    return True\n</code></pre>"},{"location":"guides/creating_abilities/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<pre><code>async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n    try:\n        result = await self._do_work(request)\n        return self._success_response(request, result)\n    except ApiError as e:\n        logger.error(\"api_error\", error=str(e))\n        return self._error_response(request, f\"API error: {str(e)}\")\n    except Exception as e:\n        logger.error(\"unexpected_error\", error=str(e))\n        return self._error_response(request, \"An unexpected error occurred\")\n</code></pre>"},{"location":"guides/creating_abilities/#4-log-appropriately","title":"4. Log Appropriately","text":"<pre><code>from bruno_core.utils.logging import get_logger\n\nlogger = get_logger(__name__)\n\nasync def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n    logger.info(\"ability_executing\", \n                ability=self._metadata.name,\n                action=request.action,\n                user_id=request.user_id)\n\n    result = await self._do_work(request)\n\n    logger.info(\"ability_executed\",\n                ability=self._metadata.name,\n                success=True)\n\n    return self._success_response(request, result)\n</code></pre>"},{"location":"guides/creating_abilities/#5-document-thoroughly","title":"5. Document Thoroughly","text":"<pre><code>class DocumentedAbility(BaseAbility):\n    \"\"\"\n    A well-documented ability.\n\n    This ability does X, Y, and Z. It requires API credentials\n    and has the following limitations:\n    - Rate limit: 100 requests/hour\n    - Max payload: 1MB\n\n    Example:\n        &gt;&gt;&gt; ability = DocumentedAbility(api_key=\"...\")\n        &gt;&gt;&gt; await ability.initialize()\n        &gt;&gt;&gt; response = await ability.execute(request)\n    \"\"\"\n\n    async def execute_action(self, request: AbilityRequest) -&gt; AbilityResponse:\n        \"\"\"\n        Execute the ability action.\n\n        Args:\n            request: Ability request with action and parameters\n\n        Returns:\n            AbilityResponse with success status and data\n\n        Raises:\n            ApiError: If API call fails\n            ValidationError: If parameters are invalid\n        \"\"\"\n        pass\n</code></pre>"},{"location":"guides/creating_abilities/#examples","title":"Examples","text":"<p>See the examples directory for complete, working examples of: - Basic ability - API integration ability - Database ability - File operations ability - Scheduled task ability</p>"},{"location":"guides/creating_abilities/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/creating_abilities/#ability-not-detected","title":"Ability Not Detected","text":"<p>Ensure: 1. Ability is registered with assistant 2. get_supported_actions() returns correct action names 3. Action keywords appear in user messages</p>"},{"location":"guides/creating_abilities/#validation-failures","title":"Validation Failures","text":"<p>Check: 1. Required parameters are present 2. Parameter types match metadata 3. allowed_values constraints are met</p>"},{"location":"guides/creating_abilities/#execution-errors","title":"Execution Errors","text":"<p>Debug by: 1. Adding logging statements 2. Checking health_check() output 3. Testing with simple inputs first 4. Reviewing error messages in response</p> <p>For more examples, see the examples directory.</p>"},{"location":"guides/custom_llm/","title":"Custom LLM Provider Guide","text":"<p>This guide shows how to integrate custom language model providers with bruno-core.</p>"},{"location":"guides/custom_llm/#overview","title":"Overview","text":"<p>Bruno-core uses the <code>LLMInterface</code> to abstract language model interactions. You can implement this interface to integrate any LLM provider:</p> <ul> <li>Cloud APIs: OpenAI, Anthropic Claude, Google PaLM, Cohere</li> <li>Self-hosted: Ollama, LM Studio, text-generation-webui</li> <li>Custom models: Your own fine-tuned models</li> <li>Hybrid: Multiple providers with fallback logic</li> </ul>"},{"location":"guides/custom_llm/#llminterface-contract","title":"LLMInterface Contract","text":"<pre><code>from bruno_core.interfaces import LLMInterface\nfrom bruno_core.models import Message\nfrom typing import AsyncIterator\n\nclass CustomLLM(LLMInterface):\n    async def generate(self, messages: list[Message], **kwargs) -&gt; str:\n        \"\"\"Generate a complete response.\"\"\"\n        raise NotImplementedError\n\n    async def stream(self, messages: list[Message], **kwargs) -&gt; AsyncIterator[str]:\n        \"\"\"Stream response tokens.\"\"\"\n        raise NotImplementedError\n\n    def get_token_count(self, text: str) -&gt; int:\n        \"\"\"Estimate token count.\"\"\"\n        raise NotImplementedError\n\n    def list_models(self) -&gt; list[str]:\n        \"\"\"List available models.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"guides/custom_llm/#basic-implementation","title":"Basic Implementation","text":""},{"location":"guides/custom_llm/#simple-llm-provider","title":"Simple LLM Provider","text":"<pre><code>import asyncio\nfrom bruno_core.interfaces import LLMInterface\nfrom bruno_core.models import Message, MessageRole\n\nclass SimpleLLM(LLMInterface):\n    \"\"\"Minimal LLM implementation.\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"gpt-4\"):\n        self.api_key = api_key\n        self.model = model\n\n    async def generate(self, messages: list[Message], **kwargs) -&gt; str:\n        # Format messages for your API\n        formatted = self._format_messages(messages)\n\n        # Make API call\n        response = await self._call_api(formatted, **kwargs)\n\n        return response\n\n    async def stream(self, messages: list[Message], **kwargs):\n        formatted = self._format_messages(messages)\n\n        async for token in self._stream_api(formatted, **kwargs):\n            yield token\n\n    def get_token_count(self, text: str) -&gt; int:\n        # Simple approximation\n        return len(text.split())\n\n    def list_models(self) -&gt; list[str]:\n        return [\"gpt-4\", \"gpt-3.5-turbo\"]\n\n    def _format_messages(self, messages: list[Message]) -&gt; list[dict]:\n        return [\n            {\"role\": msg.role.value, \"content\": msg.content}\n            for msg in messages\n        ]\n\n    async def _call_api(self, messages: list[dict], **kwargs) -&gt; str:\n        # Your API call logic here\n        pass\n\n    async def _stream_api(self, messages: list[dict], **kwargs):\n        # Your streaming API call logic here\n        yield \"token\"\n</code></pre>"},{"location":"guides/custom_llm/#real-world-implementations","title":"Real-World Implementations","text":""},{"location":"guides/custom_llm/#openai-provider","title":"OpenAI Provider","text":"<pre><code>from openai import AsyncOpenAI\nfrom bruno_core.interfaces import LLMInterface\nfrom bruno_core.models import Message\n\nclass OpenAILLM(LLMInterface):\n    \"\"\"OpenAI GPT integration.\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"gpt-4\"):\n        self.client = AsyncOpenAI(api_key=api_key)\n        self.model = model\n\n    async def generate(self, messages: list[Message], **kwargs) -&gt; str:\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\"role\": msg.role.value, \"content\": msg.content}\n                for msg in messages\n            ],\n            **kwargs\n        )\n        return response.choices[0].message.content\n\n    async def stream(self, messages: list[Message], **kwargs):\n        stream = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\"role\": msg.role.value, \"content\": msg.content}\n                for msg in messages\n            ],\n            stream=True,\n            **kwargs\n        )\n\n        async for chunk in stream:\n            if chunk.choices[0].delta.content:\n                yield chunk.choices[0].delta.content\n\n    def get_token_count(self, text: str) -&gt; int:\n        import tiktoken\n        encoding = tiktoken.encoding_for_model(self.model)\n        return len(encoding.encode(text))\n\n    def list_models(self) -&gt; list[str]:\n        return [\"gpt-4\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"]\n</code></pre>"},{"location":"guides/custom_llm/#anthropic-claude-provider","title":"Anthropic Claude Provider","text":"<pre><code>from anthropic import AsyncAnthropic\nfrom bruno_core.interfaces import LLMInterface\nfrom bruno_core.models import Message, MessageRole\n\nclass ClaudeLLM(LLMInterface):\n    \"\"\"Anthropic Claude integration.\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"claude-3-opus-20240229\"):\n        self.client = AsyncAnthropic(api_key=api_key)\n        self.model = model\n\n    async def generate(self, messages: list[Message], **kwargs) -&gt; str:\n        # Separate system message if present\n        system = None\n        claude_messages = []\n\n        for msg in messages:\n            if msg.role == MessageRole.SYSTEM:\n                system = msg.content\n            else:\n                claude_messages.append({\n                    \"role\": msg.role.value,\n                    \"content\": msg.content\n                })\n\n        response = await self.client.messages.create(\n            model=self.model,\n            messages=claude_messages,\n            system=system,\n            max_tokens=kwargs.get(\"max_tokens\", 1024),\n            **kwargs\n        )\n\n        return response.content[0].text\n\n    async def stream(self, messages: list[Message], **kwargs):\n        system = None\n        claude_messages = []\n\n        for msg in messages:\n            if msg.role == MessageRole.SYSTEM:\n                system = msg.content\n            else:\n                claude_messages.append({\n                    \"role\": msg.role.value,\n                    \"content\": msg.content\n                })\n\n        async with self.client.messages.stream(\n            model=self.model,\n            messages=claude_messages,\n            system=system,\n            max_tokens=kwargs.get(\"max_tokens\", 1024),\n            **kwargs\n        ) as stream:\n            async for text in stream.text_stream:\n                yield text\n\n    def get_token_count(self, text: str) -&gt; int:\n        # Claude doesn't have a public tokenizer yet\n        # Use approximation: ~1 token per 4 characters\n        return len(text) // 4\n\n    def list_models(self) -&gt; list[str]:\n        return [\n            \"claude-3-opus-20240229\",\n            \"claude-3-sonnet-20240229\",\n            \"claude-3-haiku-20240307\"\n        ]\n</code></pre>"},{"location":"guides/custom_llm/#ollama-local-provider","title":"Ollama Local Provider","text":"<pre><code>import aiohttp\nfrom bruno_core.interfaces import LLMInterface\nfrom bruno_core.models import Message\n\nclass OllamaLLM(LLMInterface):\n    \"\"\"Ollama local LLM integration.\"\"\"\n\n    def __init__(self, base_url: str = \"http://localhost:11434\", model: str = \"llama2\"):\n        self.base_url = base_url\n        self.model = model\n\n    async def generate(self, messages: list[Message], **kwargs) -&gt; str:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{self.base_url}/api/chat\",\n                json={\n                    \"model\": self.model,\n                    \"messages\": [\n                        {\"role\": msg.role.value, \"content\": msg.content}\n                        for msg in messages\n                    ],\n                    \"stream\": False,\n                    **kwargs\n                }\n            ) as response:\n                data = await response.json()\n                return data[\"message\"][\"content\"]\n\n    async def stream(self, messages: list[Message], **kwargs):\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{self.base_url}/api/chat\",\n                json={\n                    \"model\": self.model,\n                    \"messages\": [\n                        {\"role\": msg.role.value, \"content\": msg.content}\n                        for msg in messages\n                    ],\n                    \"stream\": True,\n                    **kwargs\n                }\n            ) as response:\n                async for line in response.content:\n                    if line:\n                        import json\n                        data = json.loads(line)\n                        if \"message\" in data:\n                            yield data[\"message\"][\"content\"]\n\n    def get_token_count(self, text: str) -&gt; int:\n        return len(text.split())\n\n    def list_models(self) -&gt; list[str]:\n        import requests\n        response = requests.get(f\"{self.base_url}/api/tags\")\n        return [model[\"name\"] for model in response.json()[\"models\"]]\n</code></pre>"},{"location":"guides/custom_llm/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/custom_llm/#rate-limiting","title":"Rate Limiting","text":"<pre><code>import time\nimport asyncio\nfrom bruno_core.interfaces import LLMInterface\n\nclass RateLimitedLLM(LLMInterface):\n    \"\"\"LLM with rate limiting.\"\"\"\n\n    def __init__(self, base_llm: LLMInterface, requests_per_minute: int = 60):\n        self.base_llm = base_llm\n        self.rpm = requests_per_minute\n        self.last_request = 0.0\n\n    async def _wait_for_rate_limit(self):\n        min_interval = 60.0 / self.rpm\n        elapsed = time.time() - self.last_request\n\n        if elapsed &lt; min_interval:\n            await asyncio.sleep(min_interval - elapsed)\n\n        self.last_request = time.time()\n\n    async def generate(self, messages, **kwargs):\n        await self._wait_for_rate_limit()\n        return await self.base_llm.generate(messages, **kwargs)\n\n    async def stream(self, messages, **kwargs):\n        await self._wait_for_rate_limit()\n        async for token in self.base_llm.stream(messages, **kwargs):\n            yield token\n\n    def get_token_count(self, text: str) -&gt; int:\n        return self.base_llm.get_token_count(text)\n\n    def list_models(self) -&gt; list[str]:\n        return self.base_llm.list_models()\n</code></pre>"},{"location":"guides/custom_llm/#retry-logic","title":"Retry Logic","text":"<pre><code>import asyncio\nfrom bruno_core.interfaces import LLMInterface\n\nclass RetryLLM(LLMInterface):\n    \"\"\"LLM with automatic retries.\"\"\"\n\n    def __init__(self, base_llm: LLMInterface, max_retries: int = 3):\n        self.base_llm = base_llm\n        self.max_retries = max_retries\n\n    async def generate(self, messages, **kwargs):\n        last_error = None\n\n        for attempt in range(self.max_retries):\n            try:\n                return await self.base_llm.generate(messages, **kwargs)\n            except Exception as e:\n                last_error = e\n                if attempt &lt; self.max_retries - 1:\n                    await asyncio.sleep(2 ** attempt)  # Exponential backoff\n                    continue\n\n        raise Exception(f\"Failed after {self.max_retries} attempts: {last_error}\")\n\n    async def stream(self, messages, **kwargs):\n        async for token in self.base_llm.stream(messages, **kwargs):\n            yield token\n\n    def get_token_count(self, text: str) -&gt; int:\n        return self.base_llm.get_token_count(text)\n\n    def list_models(self) -&gt; list[str]:\n        return self.base_llm.list_models()\n</code></pre>"},{"location":"guides/custom_llm/#multi-provider-fallback","title":"Multi-Provider Fallback","text":"<pre><code>from bruno_core.interfaces import LLMInterface\nfrom typing import List\n\nclass FallbackLLM(LLMInterface):\n    \"\"\"Try multiple providers in order.\"\"\"\n\n    def __init__(self, providers: List[LLMInterface]):\n        self.providers = providers\n\n    async def generate(self, messages, **kwargs):\n        last_error = None\n\n        for provider in self.providers:\n            try:\n                return await provider.generate(messages, **kwargs)\n            except Exception as e:\n                last_error = e\n                continue\n\n        raise Exception(f\"All providers failed. Last error: {last_error}\")\n\n    async def stream(self, messages, **kwargs):\n        for provider in self.providers:\n            try:\n                async for token in provider.stream(messages, **kwargs):\n                    yield token\n                return\n            except Exception:\n                continue\n\n    def get_token_count(self, text: str) -&gt; int:\n        return self.providers[0].get_token_count(text)\n\n    def list_models(self) -&gt; list[str]:\n        models = []\n        for provider in self.providers:\n            models.extend(provider.list_models())\n        return list(set(models))\n</code></pre>"},{"location":"guides/custom_llm/#usage","title":"Usage","text":""},{"location":"guides/custom_llm/#basic-usage","title":"Basic Usage","text":"<pre><code>from bruno_core.base import BaseAssistant\n\n# Create LLM\nllm = OpenAILLM(api_key=\"your-key\")\n\n# Create assistant\nassistant = BaseAssistant(llm=llm, memory=memory)\nawait assistant.initialize()\n</code></pre>"},{"location":"guides/custom_llm/#with-rate-limiting","title":"With Rate Limiting","text":"<pre><code>base_llm = OpenAILLM(api_key=\"your-key\")\nllm = RateLimitedLLM(base_llm, requests_per_minute=30)\n\nassistant = BaseAssistant(llm=llm, memory=memory)\n</code></pre>"},{"location":"guides/custom_llm/#with-fallback","title":"With Fallback","text":"<pre><code>primary = OpenAILLM(api_key=\"openai-key\")\nfallback = ClaudeLLM(api_key=\"claude-key\")\nlocal = OllamaLLM()\n\nllm = FallbackLLM([primary, fallback, local])\n\nassistant = BaseAssistant(llm=llm, memory=memory)\n</code></pre>"},{"location":"guides/custom_llm/#testing","title":"Testing","text":"<pre><code>import pytest\nfrom bruno_core.models import Message, MessageRole\n\n@pytest.mark.asyncio\nasync def test_custom_llm():\n    llm = CustomLLM(api_key=\"test-key\")\n\n    messages = [\n        Message(role=MessageRole.USER, content=\"Hello\")\n    ]\n\n    response = await llm.generate(messages)\n    assert isinstance(response, str)\n    assert len(response) &gt; 0\n\n    # Test streaming\n    tokens = []\n    async for token in llm.stream(messages):\n        tokens.append(token)\n\n    assert len(tokens) &gt; 0\n</code></pre>"},{"location":"guides/custom_llm/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling: Always handle API errors gracefully</li> <li>Rate Limiting: Implement rate limiting for cloud APIs</li> <li>Retries: Add retry logic with exponential backoff</li> <li>Timeouts: Set reasonable timeouts for API calls</li> <li>Token Counting: Use provider-specific tokenizers when available</li> <li>Streaming: Implement streaming for better UX</li> <li>Context Management: Handle context window limits</li> <li>Cost Tracking: Log token usage for cost monitoring</li> </ol>"},{"location":"guides/custom_llm/#next-steps","title":"Next Steps","text":"<ul> <li>Memory Backends Guide</li> <li>Creating Abilities Guide</li> <li>API Reference</li> </ul>"},{"location":"guides/memory_backends/","title":"Memory Backends Guide","text":"<p>This guide shows how to implement custom memory backends for bruno-core.</p>"},{"location":"guides/memory_backends/#overview","title":"Overview","text":"<p>Bruno-core uses the <code>MemoryInterface</code> to abstract conversation storage and retrieval. You can implement this interface to integrate any storage backend:</p> <ul> <li>In-Memory: Simple dict-based storage for development</li> <li>SQL Databases: PostgreSQL, MySQL, SQLite</li> <li>NoSQL Databases: MongoDB, DynamoDB</li> <li>Vector Databases: Pinecone, Weaviate, ChromaDB, Qdrant</li> <li>Cache Systems: Redis, Memcached</li> <li>Hybrid: Combine multiple backends</li> </ul>"},{"location":"guides/memory_backends/#memoryinterface-contract","title":"MemoryInterface Contract","text":"<pre><code>from bruno_core.interfaces import MemoryInterface\nfrom bruno_core.models import Message, MemoryEntry\nfrom typing import Optional\n\nclass CustomMemory(MemoryInterface):\n    async def store_message(\n        self,\n        message: Message,\n        user_id: str,\n        conversation_id: str\n    ):\n        \"\"\"Store a message.\"\"\"\n        raise NotImplementedError\n\n    async def retrieve_context(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 10,\n        conversation_id: Optional[str] = None\n    ) -&gt; list[Message]:\n        \"\"\"Retrieve recent context.\"\"\"\n        raise NotImplementedError\n\n    async def search_memories(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 5\n    ) -&gt; list[MemoryEntry]:\n        \"\"\"Search memories semantically.\"\"\"\n        raise NotImplementedError\n\n    async def clear_conversation(self, user_id: str, conversation_id: str):\n        \"\"\"Clear a conversation.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"guides/memory_backends/#basic-implementations","title":"Basic Implementations","text":""},{"location":"guides/memory_backends/#in-memory-storage","title":"In-Memory Storage","text":"<pre><code>from bruno_core.interfaces import MemoryInterface\nfrom bruno_core.models import Message, MemoryEntry\nfrom typing import Optional\nfrom collections import defaultdict\n\nclass InMemoryStorage(MemoryInterface):\n    \"\"\"Simple dictionary-based storage.\"\"\"\n\n    def __init__(self):\n        # Structure: {user_id: {conversation_id: [messages]}}\n        self.storage = defaultdict(lambda: defaultdict(list))\n\n    async def store_message(\n        self,\n        message: Message,\n        user_id: str,\n        conversation_id: str\n    ):\n        self.storage[user_id][conversation_id].append(message)\n\n    async def retrieve_context(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 10,\n        conversation_id: Optional[str] = None\n    ) -&gt; list[Message]:\n        if conversation_id:\n            messages = self.storage[user_id][conversation_id]\n        else:\n            # Merge all conversations\n            messages = []\n            for conv_messages in self.storage[user_id].values():\n                messages.extend(conv_messages)\n\n        return messages[-limit:]\n\n    async def search_memories(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 5\n    ) -&gt; list[MemoryEntry]:\n        results = []\n\n        for conv_id, messages in self.storage[user_id].items():\n            for msg in messages:\n                if query.lower() in msg.content.lower():\n                    entry = MemoryEntry(\n                        id=f\"{user_id}:{conv_id}:{msg.timestamp}\",\n                        user_id=user_id,\n                        content=msg.content,\n                        metadata={\"conversation_id\": conv_id},\n                        timestamp=msg.timestamp\n                    )\n                    results.append(entry)\n\n        return results[:limit]\n\n    async def clear_conversation(self, user_id: str, conversation_id: str):\n        if user_id in self.storage:\n            self.storage[user_id].pop(conversation_id, None)\n</code></pre>"},{"location":"guides/memory_backends/#sql-database-implementations","title":"SQL Database Implementations","text":""},{"location":"guides/memory_backends/#postgresql-backend","title":"PostgreSQL Backend","text":"<pre><code>import asyncpg\nfrom bruno_core.interfaces import MemoryInterface\nfrom bruno_core.models import Message, MemoryEntry, MessageRole\nfrom typing import Optional\nfrom datetime import datetime\n\nclass PostgresMemory(MemoryInterface):\n    \"\"\"PostgreSQL-based memory storage.\"\"\"\n\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n        self.pool = None\n\n    async def initialize(self):\n        \"\"\"Create connection pool and tables.\"\"\"\n        self.pool = await asyncpg.create_pool(self.connection_string)\n\n        async with self.pool.acquire() as conn:\n            await conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS messages (\n                    id SERIAL PRIMARY KEY,\n                    user_id VARCHAR(255) NOT NULL,\n                    conversation_id VARCHAR(255) NOT NULL,\n                    role VARCHAR(50) NOT NULL,\n                    content TEXT NOT NULL,\n                    timestamp TIMESTAMP NOT NULL,\n                    metadata JSONB,\n                    INDEX idx_user_conv (user_id, conversation_id),\n                    INDEX idx_timestamp (timestamp)\n                )\n            \"\"\")\n\n    async def store_message(\n        self,\n        message: Message,\n        user_id: str,\n        conversation_id: str\n    ):\n        async with self.pool.acquire() as conn:\n            await conn.execute(\"\"\"\n                INSERT INTO messages (user_id, conversation_id, role, content, timestamp, metadata)\n                VALUES ($1, $2, $3, $4, $5, $6)\n            \"\"\", user_id, conversation_id, message.role.value, message.content,\n                message.timestamp, message.metadata or {})\n\n    async def retrieve_context(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 10,\n        conversation_id: Optional[str] = None\n    ) -&gt; list[Message]:\n        async with self.pool.acquire() as conn:\n            if conversation_id:\n                rows = await conn.fetch(\"\"\"\n                    SELECT role, content, timestamp, metadata\n                    FROM messages\n                    WHERE user_id = $1 AND conversation_id = $2\n                    ORDER BY timestamp DESC\n                    LIMIT $3\n                \"\"\", user_id, conversation_id, limit)\n            else:\n                rows = await conn.fetch(\"\"\"\n                    SELECT role, content, timestamp, metadata\n                    FROM messages\n                    WHERE user_id = $1\n                    ORDER BY timestamp DESC\n                    LIMIT $2\n                \"\"\", user_id, limit)\n\n        messages = []\n        for row in reversed(rows):\n            messages.append(Message(\n                role=MessageRole(row['role']),\n                content=row['content'],\n                timestamp=row['timestamp'],\n                metadata=row['metadata']\n            ))\n\n        return messages\n\n    async def search_memories(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 5\n    ) -&gt; list[MemoryEntry]:\n        async with self.pool.acquire() as conn:\n            rows = await conn.fetch(\"\"\"\n                SELECT id, content, timestamp, conversation_id\n                FROM messages\n                WHERE user_id = $1 AND content ILIKE $2\n                ORDER BY timestamp DESC\n                LIMIT $3\n            \"\"\", user_id, f\"%{query}%\", limit)\n\n        return [\n            MemoryEntry(\n                id=str(row['id']),\n                user_id=user_id,\n                content=row['content'],\n                timestamp=row['timestamp'],\n                metadata={\"conversation_id\": row['conversation_id']}\n            )\n            for row in rows\n        ]\n\n    async def clear_conversation(self, user_id: str, conversation_id: str):\n        async with self.pool.acquire() as conn:\n            await conn.execute(\"\"\"\n                DELETE FROM messages\n                WHERE user_id = $1 AND conversation_id = $2\n            \"\"\", user_id, conversation_id)\n\n    async def shutdown(self):\n        \"\"\"Close connection pool.\"\"\"\n        if self.pool:\n            await self.pool.close()\n</code></pre>"},{"location":"guides/memory_backends/#sqlite-backend","title":"SQLite Backend","text":"<pre><code>import aiosqlite\nfrom bruno_core.interfaces import MemoryInterface\nfrom bruno_core.models import Message, MemoryEntry, MessageRole\nfrom datetime import datetime\nimport json\n\nclass SQLiteMemory(MemoryInterface):\n    \"\"\"SQLite-based memory storage.\"\"\"\n\n    def __init__(self, db_path: str = \"bruno_memory.db\"):\n        self.db_path = db_path\n        self.conn = None\n\n    async def initialize(self):\n        \"\"\"Create database and tables.\"\"\"\n        self.conn = await aiosqlite.connect(self.db_path)\n\n        await self.conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS messages (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                user_id TEXT NOT NULL,\n                conversation_id TEXT NOT NULL,\n                role TEXT NOT NULL,\n                content TEXT NOT NULL,\n                timestamp TEXT NOT NULL,\n                metadata TEXT\n            )\n        \"\"\")\n\n        await self.conn.execute(\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_user_conv\n            ON messages(user_id, conversation_id)\n        \"\"\")\n\n        await self.conn.commit()\n\n    async def store_message(\n        self,\n        message: Message,\n        user_id: str,\n        conversation_id: str\n    ):\n        await self.conn.execute(\"\"\"\n            INSERT INTO messages (user_id, conversation_id, role, content, timestamp, metadata)\n            VALUES (?, ?, ?, ?, ?, ?)\n        \"\"\", (user_id, conversation_id, message.role.value, message.content,\n              message.timestamp.isoformat(), json.dumps(message.metadata or {})))\n\n        await self.conn.commit()\n\n    async def retrieve_context(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 10,\n        conversation_id: Optional[str] = None\n    ) -&gt; list[Message]:\n        if conversation_id:\n            cursor = await self.conn.execute(\"\"\"\n                SELECT role, content, timestamp, metadata\n                FROM messages\n                WHERE user_id = ? AND conversation_id = ?\n                ORDER BY timestamp DESC\n                LIMIT ?\n            \"\"\", (user_id, conversation_id, limit))\n        else:\n            cursor = await self.conn.execute(\"\"\"\n                SELECT role, content, timestamp, metadata\n                FROM messages\n                WHERE user_id = ?\n                ORDER BY timestamp DESC\n                LIMIT ?\n            \"\"\", (user_id, limit))\n\n        rows = await cursor.fetchall()\n\n        messages = []\n        for row in reversed(rows):\n            messages.append(Message(\n                role=MessageRole(row[0]),\n                content=row[1],\n                timestamp=datetime.fromisoformat(row[2]),\n                metadata=json.loads(row[3])\n            ))\n\n        return messages\n\n    async def search_memories(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 5\n    ) -&gt; list[MemoryEntry]:\n        cursor = await self.conn.execute(\"\"\"\n            SELECT id, content, timestamp, conversation_id\n            FROM messages\n            WHERE user_id = ? AND content LIKE ?\n            ORDER BY timestamp DESC\n            LIMIT ?\n        \"\"\", (user_id, f\"%{query}%\", limit))\n\n        rows = await cursor.fetchall()\n\n        return [\n            MemoryEntry(\n                id=str(row[0]),\n                user_id=user_id,\n                content=row[1],\n                timestamp=datetime.fromisoformat(row[2]),\n                metadata={\"conversation_id\": row[3]}\n            )\n            for row in rows\n        ]\n\n    async def clear_conversation(self, user_id: str, conversation_id: str):\n        await self.conn.execute(\"\"\"\n            DELETE FROM messages\n            WHERE user_id = ? AND conversation_id = ?\n        \"\"\", (user_id, conversation_id))\n\n        await self.conn.commit()\n\n    async def shutdown(self):\n        \"\"\"Close database connection.\"\"\"\n        if self.conn:\n            await self.conn.close()\n</code></pre>"},{"location":"guides/memory_backends/#vector-database-implementations","title":"Vector Database Implementations","text":""},{"location":"guides/memory_backends/#chromadb-backend-semantic-search","title":"ChromaDB Backend (Semantic Search)","text":"<pre><code>import chromadb\nfrom chromadb.config import Settings\nfrom bruno_core.interfaces import MemoryInterface, EmbeddingInterface\nfrom bruno_core.models import Message, MemoryEntry, MessageRole\nfrom typing import Optional\nfrom datetime import datetime\nimport json\n\nclass ChromaMemory(MemoryInterface):\n    \"\"\"ChromaDB-based semantic memory.\"\"\"\n\n    def __init__(\n        self,\n        embedding_model: EmbeddingInterface,\n        persist_directory: str = \"./chroma_db\"\n    ):\n        self.embedding_model = embedding_model\n        self.client = chromadb.Client(Settings(\n            chroma_db_impl=\"duckdb+parquet\",\n            persist_directory=persist_directory\n        ))\n        self.collection = self.client.get_or_create_collection(\"bruno_memories\")\n\n    async def store_message(\n        self,\n        message: Message,\n        user_id: str,\n        conversation_id: str\n    ):\n        # Generate embedding\n        embedding = await self.embedding_model.embed(message.content)\n\n        # Store in ChromaDB\n        doc_id = f\"{user_id}:{conversation_id}:{message.timestamp.isoformat()}\"\n\n        self.collection.add(\n            ids=[doc_id],\n            embeddings=[embedding],\n            documents=[message.content],\n            metadatas=[{\n                \"user_id\": user_id,\n                \"conversation_id\": conversation_id,\n                \"role\": message.role.value,\n                \"timestamp\": message.timestamp.isoformat(),\n                \"metadata\": json.dumps(message.metadata or {})\n            }]\n        )\n\n    async def retrieve_context(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 10,\n        conversation_id: Optional[str] = None\n    ) -&gt; list[Message]:\n        # Generate query embedding\n        query_embedding = await self.embedding_model.embed(query)\n\n        # Build filter\n        where_filter = {\"user_id\": user_id}\n        if conversation_id:\n            where_filter[\"conversation_id\"] = conversation_id\n\n        # Search\n        results = self.collection.query(\n            query_embeddings=[query_embedding],\n            n_results=limit,\n            where=where_filter\n        )\n\n        # Convert to messages\n        messages = []\n        for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):\n            messages.append(Message(\n                role=MessageRole(metadata['role']),\n                content=doc,\n                timestamp=datetime.fromisoformat(metadata['timestamp']),\n                metadata=json.loads(metadata['metadata'])\n            ))\n\n        return messages\n\n    async def search_memories(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 5\n    ) -&gt; list[MemoryEntry]:\n        query_embedding = await self.embedding_model.embed(query)\n\n        results = self.collection.query(\n            query_embeddings=[query_embedding],\n            n_results=limit,\n            where={\"user_id\": user_id}\n        )\n\n        memories = []\n        for doc_id, doc, metadata in zip(\n            results['ids'][0],\n            results['documents'][0],\n            results['metadatas'][0]\n        ):\n            memories.append(MemoryEntry(\n                id=doc_id,\n                user_id=user_id,\n                content=doc,\n                timestamp=datetime.fromisoformat(metadata['timestamp']),\n                metadata={\"conversation_id\": metadata['conversation_id']}\n            ))\n\n        return memories\n\n    async def clear_conversation(self, user_id: str, conversation_id: str):\n        # Get all IDs for this conversation\n        results = self.collection.get(\n            where={\n                \"user_id\": user_id,\n                \"conversation_id\": conversation_id\n            }\n        )\n\n        if results['ids']:\n            self.collection.delete(ids=results['ids'])\n</code></pre>"},{"location":"guides/memory_backends/#redis-cache-backend","title":"Redis Cache Backend","text":"<pre><code>import redis.asyncio as aioredis\nfrom bruno_core.interfaces import MemoryInterface\nfrom bruno_core.models import Message, MemoryEntry, MessageRole\nfrom typing import Optional\nimport json\n\nclass RedisMemory(MemoryInterface):\n    \"\"\"Redis-based memory cache.\"\"\"\n\n    def __init__(self, redis_url: str = \"redis://localhost\"):\n        self.redis_url = redis_url\n        self.redis = None\n\n    async def initialize(self):\n        \"\"\"Connect to Redis.\"\"\"\n        self.redis = await aioredis.from_url(self.redis_url)\n\n    async def store_message(\n        self,\n        message: Message,\n        user_id: str,\n        conversation_id: str\n    ):\n        key = f\"conv:{user_id}:{conversation_id}\"\n\n        message_data = {\n            \"role\": message.role.value,\n            \"content\": message.content,\n            \"timestamp\": message.timestamp.isoformat(),\n            \"metadata\": message.metadata or {}\n        }\n\n        # Add to list\n        await self.redis.rpush(key, json.dumps(message_data))\n\n        # Set expiration (optional)\n        await self.redis.expire(key, 86400 * 30)  # 30 days\n\n    async def retrieve_context(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 10,\n        conversation_id: Optional[str] = None\n    ) -&gt; list[Message]:\n        if conversation_id:\n            key = f\"conv:{user_id}:{conversation_id}\"\n            messages_data = await self.redis.lrange(key, -limit, -1)\n        else:\n            # Get all conversations for user\n            pattern = f\"conv:{user_id}:*\"\n            keys = []\n            async for key in self.redis.scan_iter(match=pattern):\n                keys.append(key)\n\n            messages_data = []\n            for key in keys:\n                messages_data.extend(await self.redis.lrange(key, -limit, -1))\n\n        messages = []\n        for data in messages_data:\n            msg_dict = json.loads(data)\n            messages.append(Message(\n                role=MessageRole(msg_dict['role']),\n                content=msg_dict['content'],\n                timestamp=datetime.fromisoformat(msg_dict['timestamp']),\n                metadata=msg_dict['metadata']\n            ))\n\n        return messages[-limit:]\n\n    async def search_memories(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 5\n    ) -&gt; list[MemoryEntry]:\n        # Redis doesn't have great full-text search\n        # Consider RedisSearch for production\n        pattern = f\"conv:{user_id}:*\"\n        results = []\n\n        async for key in self.redis.scan_iter(match=pattern):\n            messages_data = await self.redis.lrange(key, 0, -1)\n\n            for data in messages_data:\n                msg_dict = json.loads(data)\n                if query.lower() in msg_dict['content'].lower():\n                    results.append(MemoryEntry(\n                        id=f\"{key}:{msg_dict['timestamp']}\",\n                        user_id=user_id,\n                        content=msg_dict['content'],\n                        timestamp=datetime.fromisoformat(msg_dict['timestamp']),\n                        metadata={\"conversation_id\": key.split(\":\")[-1]}\n                    ))\n\n        return results[:limit]\n\n    async def clear_conversation(self, user_id: str, conversation_id: str):\n        key = f\"conv:{user_id}:{conversation_id}\"\n        await self.redis.delete(key)\n\n    async def shutdown(self):\n        \"\"\"Close Redis connection.\"\"\"\n        if self.redis:\n            await self.redis.close()\n</code></pre>"},{"location":"guides/memory_backends/#hybrid-backend","title":"Hybrid Backend","text":"<pre><code>from bruno_core.interfaces import MemoryInterface\nfrom bruno_core.models import Message, MemoryEntry\nfrom typing import Optional\n\nclass HybridMemory(MemoryInterface):\n    \"\"\"Combine cache and persistent storage.\"\"\"\n\n    def __init__(\n        self,\n        cache: MemoryInterface,  # e.g., Redis\n        persistent: MemoryInterface  # e.g., PostgreSQL\n    ):\n        self.cache = cache\n        self.persistent = persistent\n\n    async def store_message(\n        self,\n        message: Message,\n        user_id: str,\n        conversation_id: str\n    ):\n        # Store in both\n        await self.cache.store_message(message, user_id, conversation_id)\n        await self.persistent.store_message(message, user_id, conversation_id)\n\n    async def retrieve_context(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 10,\n        conversation_id: Optional[str] = None\n    ) -&gt; list[Message]:\n        # Try cache first\n        messages = await self.cache.retrieve_context(\n            user_id, query, limit, conversation_id\n        )\n\n        if not messages:\n            # Fallback to persistent storage\n            messages = await self.persistent.retrieve_context(\n                user_id, query, limit, conversation_id\n            )\n\n        return messages\n\n    async def search_memories(\n        self,\n        user_id: str,\n        query: str,\n        limit: int = 5\n    ) -&gt; list[MemoryEntry]:\n        # Use persistent storage for search\n        return await self.persistent.search_memories(user_id, query, limit)\n\n    async def clear_conversation(self, user_id: str, conversation_id: str):\n        await self.cache.clear_conversation(user_id, conversation_id)\n        await self.persistent.clear_conversation(user_id, conversation_id)\n</code></pre>"},{"location":"guides/memory_backends/#usage-examples","title":"Usage Examples","text":""},{"location":"guides/memory_backends/#basic-usage","title":"Basic Usage","text":"<pre><code>from bruno_core.base import BaseAssistant\n\n# PostgreSQL\nmemory = PostgresMemory(\"postgresql://user:pass@localhost/bruno\")\nawait memory.initialize()\n\n# Create assistant\nassistant = BaseAssistant(llm=llm, memory=memory)\nawait assistant.initialize()\n</code></pre>"},{"location":"guides/memory_backends/#with-semantic-search","title":"With Semantic Search","text":"<pre><code>from your_app.embeddings import YourEmbeddingModel\n\nembedding_model = YourEmbeddingModel()\nmemory = ChromaMemory(embedding_model)\n\nassistant = BaseAssistant(llm=llm, memory=memory)\n</code></pre>"},{"location":"guides/memory_backends/#hybrid-setup","title":"Hybrid Setup","text":"<pre><code>cache = RedisMemory(\"redis://localhost\")\npersistent = PostgresMemory(\"postgresql://localhost/bruno\")\n\nawait cache.initialize()\nawait persistent.initialize()\n\nmemory = HybridMemory(cache=cache, persistent=persistent)\n\nassistant = BaseAssistant(llm=llm, memory=memory)\n</code></pre>"},{"location":"guides/memory_backends/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Backend:</li> <li>Development: In-memory or SQLite</li> <li>Production: PostgreSQL + Redis cache</li> <li> <p>Semantic search: ChromaDB, Pinecone, Weaviate</p> </li> <li> <p>Implement Proper Indexing:</p> </li> <li>Index user_id + conversation_id</li> <li>Index timestamps for chronological queries</li> <li> <p>Full-text search indexes when needed</p> </li> <li> <p>Handle Errors Gracefully:</p> </li> <li>Connection failures</li> <li>Timeout handling</li> <li> <p>Retry logic</p> </li> <li> <p>Manage Data Lifecycle:</p> </li> <li>Implement TTL for temporary data</li> <li>Archive old conversations</li> <li> <p>GDPR compliance (right to deletion)</p> </li> <li> <p>Monitor Performance:</p> </li> <li>Query execution times</li> <li>Cache hit rates</li> <li>Storage usage</li> </ol>"},{"location":"guides/memory_backends/#next-steps","title":"Next Steps","text":"<ul> <li>Custom LLM Guide</li> <li>Creating Abilities Guide</li> <li>API Reference</li> </ul>"}]}